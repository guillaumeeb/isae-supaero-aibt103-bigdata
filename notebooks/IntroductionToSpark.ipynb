{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we'll look at practical use of the Spark framework using PySpark. We'll see how to distribute data, how to use the MapReduce model, and how to store and manipulate distributed data.\n",
    "\n",
    "Spark is most often used in large datacenters, GCP provides [DataProc](https://cloud.google.com/dataproc) platform which can be used for Spark. For today, however, we'll use local installations in Binder or Colab with small examples, just to see how PySpark works. To follow, you should either use the Binder link provided, install Spark locally on a Linux machine, or upload this notebook to Colab and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "+ [Use in Binder](#binder)\n",
    "+ [Local installation on Linux](#linux)\n",
    "+ [Use in Colab](#colab)\n",
    "+ [Running Spark](#running)\n",
    "+ [Warm up with RDDs](#RDDs)\n",
    "+ [Persistent RDDs](#persistent)\n",
    "+ [Distributed K-Means](#kmeans)\n",
    "+ [DataFrames and Queries](#DataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"binder\">Use in MyBinder</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of locally installing, you can also use MyBinder or any binderHub to run this notebook: \n",
    "https://mybinder.org/\n",
    "\n",
    "https://mybinder.org/v2/gh/guillaumeeb/isae-supaero-aibt103-bigdata/main?urlpath=lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"linux\"> Local installation on Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Spark-3.2.0, prebuilt for Hadoop 2.7 from the [Spark downloads page](http://spark.apache.org/downloads.html). Install the package to `/opt/`:\n",
    "\n",
    "```bash\n",
    "$ tar xvzf spark-3.2.0-preview2-bin-hadoop2.7.tgz\n",
    "$ mv spark-3.2.0-preview2-bin-hadoop2.7 /opt/spark-3.3.0\n",
    "$ ln -s /opt/spark-3.2.0 /opt/spark\n",
    "```\n",
    "\n",
    "Afterwards, you'll want to configure your `$PATH` variable, normally in your `~/.bashrc`:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch PySpark to run this notebook. If you don't have PySpark in your Python environment, install it. You'll want to set some environment variables (as your normal user) for PySpark so that it automatically runs Jupyter.\n",
    "\n",
    "```bash\n",
    "$ pip install pyspark pyarrow\n",
    "$ export PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "$ export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --pylab inline\"\n",
    "$ pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"colab\">Use in Colab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of locally installing, you can also upload this notebook to Colab. This may be a bit slower than local installations. Uncomment (`ctrl /`) and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://apache.mirrors.benatherton.com/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "# !pip install py4j pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`findspark` will help find the Spark installation, and `py4j` lets us access Java objects. We'll see environment varibles like in the local installation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll create the Spark instance and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init(\"spark-3.0.0-preview2-bin-hadoop2.7\")# SPARK_HOMEfrom pyspark.sql import SparkSession\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"running\">Running Spark</id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can run the following line, you've properly installed and started Spark. This will show us the SparkContext `sc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the Spark version (`v3.2.0` is the last at the time of this writing) and a link to the Spark UI, a dashboard which lets us monitor Spark's activity.\n",
    "\n",
    "Spark UI won't be available with Binder or Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"RDDs\">Warm up with RDDs</a>\n",
    "\n",
    "We'll start with an example of manipulating Spark's [Resilient Distributed Datasets](https://spark.apache.org/docs/latest/rdd-programming-guide.html). We'll parallelize Python objects, but we could also use these tools to manipulate data stored in shared filesystems such as HDFS or HBase. Specifically, we'll compute pi using a [Monte Carlo Simulation](https://en.wikipedia.org/wiki/Monte_Carlo_method).\n",
    "\n",
    "![alt text](http://www.physics.smu.edu/fattarus/pi.png \"Pi simulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "\n",
    "points = sc.parallelize(range(n_points))\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our python object `range(n_points)` has been converted to a Scala object through the PySpark context `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def generate_random_pt(_):\n",
    "    x = random() * 2 - 1 # -> Rnd number between -1 and 1\n",
    "    y = random() * 2 - 1\n",
    "    return x, y\n",
    "\n",
    "def is_inside_unary_circle(t):\n",
    "    (x, y) = t\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "points = points.map(generate_random_pt)\n",
    "\n",
    "inside_points = points.filter(is_inside_unary_circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we `map` the function `generate_random_pt` to our `points` RDD, applying it to each element. We then filter these points, `(x, y)` coordinates between -1 and 1, based on if they fit in a unary circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example point : {}\".format(inside_points.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By counting the number of points inside the circle, divided by the total number of points, we can get an estimation of the ratio between circle area and square area. Multiplying the square area surface with this ratio should give us pi:\n",
    "\n",
    "Surface inside circle = pi * r^2 = pi\n",
    "\n",
    "Surface inside circle = (number of points inside / total number of points) * square surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_circle_points = inside_points.count()\n",
    "overall_area = (1 - -1) * (1 - -1)\n",
    "print(\"Pi estimation is {}\".format(overall_area * inside_circle_points\n",
    "                                   / float(n_points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the number of points, we get a better estimation. With Spark parallelization, this goes faster than if we were doing it serially. You can watch your Spark UI dashboard when launching this to see the computation live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 10000\n",
    "points = sc.parallelize(range(n_points))\n",
    "points = points.map(generate_random_pt)\n",
    "inside_points = points.filter(is_inside_unary_circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_circle_points = inside_points.count()\n",
    "print( \"Pi estimation is {}\".format(overall_area*inside_circle_points/float(n_points)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Reflection:\n",
    "    <ul>\n",
    "        <li>Monte Carlo analysis like this is considered <i>embarrasingly parallel</i>. What about the function we defined makes it easy to run in parallel?</li>\n",
    "        <li>Re-run the previous cell, which is just the <code>count</code> function. Why does this change the result? Actually since Spark last version, this does not!!</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"persistent\">Persistent RDDs</a>\n",
    "\n",
    "In Spark, there are two classes of operations: *transformations* like `map` which create a new dataset and *actions* like `count` which return a value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials=1000000\n",
    "n_throws=100\n",
    "\n",
    "pil_data = sc.parallelize(range(n_trials))\n",
    "\n",
    "def generate_play():\n",
    "    return \"heads\" if random()>0.5 else \"tails\"\n",
    "\n",
    "def generate_game(_):\n",
    "    return [generate_play() for _ in range(n_throws)]\n",
    "\n",
    "pil_data = pil_data.map(generate_game)\n",
    "\n",
    "game = pil_data.first()\n",
    "print(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first(game):\n",
    "    return game[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Heads \", pil_data.map(get_first).filter(lambda res: res==\"heads\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a MapReduce model to get the first flip of each game and then filter the games based on if the first flip is a heads. We can also count the number of tails flips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Tails \", pil_data.map(get_first).filter(lambda res: res==\"tails\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two reduce operations, counting the heads and tails, use the same map operation. To save the results from this mapping, we use `persist`. This will also speed up the computation, since the `map` function `get_first` is only applied once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cached_rdd = pil_data.map(get_first).persist()\n",
    "print(\"Heads \", cached_rdd.filter(lambda res: res==\"heads\").count())\n",
    "print(\"Tails \", cached_rdd.filter(lambda res: res==\"tails\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Exercise:\n",
    "    <br>\n",
    "    Write functions which count if the majority of flips in a single game were heads or tails by defining <code>f_count</code>, <code>heads_filter</code>, and <code>tails_filter</code>. Compare the speed of running these with a persistent RDD. You might want to lower <code>n_trials</code> to speed up testing.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_count(games):\n",
    "    ## Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_count(['heads', 'heads', 'tails', 'heads', 'heads', 'tails', 'tails', 'tails', 'tails', 'heads', 'heads', 'heads']) == [7, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads_filter(counts):\n",
    "    ## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_filter([53,47]) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tails_filter(counts):\n",
    "    ## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tails_filter([53,47]) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pil_data.map(f_count).filter(heads_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pil_data.map(f_count).filter(tails_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cached_rdd = pil_data.map(f_count)\n",
    "print('Heads: ', cached_rdd.filter(heads_filter).count())\n",
    "cached_rdd.persist()\n",
    "print('Tails: ', cached_rdd.filter(tails_filter).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Reflection:\n",
    "    <ul>\n",
    "        <li>Is <code>filter</code> a transformation or an action?</li>\n",
    "        <li>Consider the following:<br>\n",
    "<code>cached_rdd = pil_data.map(f_count)\n",
    "print('Heads: ', cached_rdd.filter(heads_filter).count())\n",
    "cached_rdd.persist()\n",
    "print('Tails: ', cached_rdd.filter(tails_filter).count())</code><br>\n",
    "            Is this a good use of persist()?\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"kmeans\">Distributed K-Means</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the MapReduce functionality we can see how some algorithms can be distributed, for example K-Means. To show this, we'll start with some randomly generated points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import numpy as np\n",
    "\n",
    "means = [0, 1, 5, -2]\n",
    "\n",
    "def generate_random_pt(_):    \n",
    "    mean = choice(means)    \n",
    "    return np.random.randn() + mean\n",
    "\n",
    "n_points = 10000\n",
    "points = sc.parallelize(range(n_points))\n",
    "points = points.map(generate_random_pt)\n",
    "points.sample(False,0.001).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our map function will calculate the nearest cluster for each point, based on the current centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to(point, centroids):\n",
    "    distances = [(point - c)**2 for c in centroids]\n",
    "    return np.argmin(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4 # Number of centroids\n",
    "centroids = []\n",
    "for i in range(N):\n",
    "    centroids.append(20 * np.random.rand() - 10)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_to(points.first(), centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our K-means algorithm will look like this:\n",
    "* Compute centroids of each cluster\n",
    "* Update centroids\n",
    "* Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First transformation:\n",
    " - Compute centroid for each datapoint\n",
    " - datapoint -> Tuple ( closest centroid Index , ( datapoint, 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = points.map(lambda p: (closest_to(p, centroids), (p, 1)))\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce By Key: Aggregate for each centroid\n",
    " - centroid Index , ( sum of datapoints , number of datapoints )\n",
    " - ( datapoint1, pop1 ) and ( datapoint2, pop2 ) => (datapoint1 + datapoint2 , pop1+pop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = rdd.reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "stats.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that at the end, \n",
    " - (cluster Index, (sum of datapoint in cluster , number of datapoints in cluster) )\n",
    " - we can compute the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat in stats.collect():\n",
    "    index, (data_sum, data_count) = stat\n",
    "    new_centroid = data_sum / data_count\n",
    "    centroids[index] = data_sum / data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll repeat this a few times to see if it converges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    rdd = points.map(lambda p: (closest_to(p, centroids), (p, 1)))\n",
    "    rdd.first()\n",
    "    stats = rdd.reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "    stats.count()\n",
    "    for stat in stats.take(N):\n",
    "        index, (data_sum, data_count) = stat\n",
    "        new_centroid = data_sum / data_count\n",
    "        centroids[index] = data_sum / data_count\n",
    "    print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like it works! However, if we really use Spark to do K-means, we should use [the Spark ML class](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/clustering.html#KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"DataFrames\">DataFrames and Queries</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark allows for structured data manipulation using SQL and DataFrames. Spark DataFrames are similar to Pandas DataFrames, and we'll look at conversion next. DataFrames can be loaded from databases, distributed filesystems, or memory. We can use `pyarrow` to convert python memory objects to Spark objects, notably Pandas DataFrames to Spark DataFrames. First we must configure Spark to use Arrow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pdf` is just a normal Pandas DataFrame, and we would not be able to do parallel manipulation on it. We'll convert it to a Spark DataFrame to allow for Spark SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert Spark DataFrames back to Pandas DataFrames, for example after heavy calculation is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "result_pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert RDDs, which we used in the last section, directly to DataFrames. We'll download some example data, the famous [wine](https://archive.ics.uci.edu/ml/datasets/Wine) dataset, and convert it to an RDD, then a DataFrame. This dataset has various chemical measurements of three different types of wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read this into our Spark context and can use a `map` function to process it. These are RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"wine.data\")\n",
    "raw = lines.map(lambda l: l.split(\",\"))\n",
    "raw.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by convering our RDD to a list of Row objects. We'll still have an RDD, but of Rows inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "wines = raw.map(\n",
    "    lambda p: Row(alcohol=int(p[0]),\n",
    "                  malic_acid=float(p[1]),\n",
    "                  ash=float(p[2]),\n",
    "                  alcalinity=float(p[3]),\n",
    "                  magnesium=float(p[4]),\n",
    "                  phenols=float(p[5]),\n",
    "                  flavanoids=float(p[6]),\n",
    "                  nonflavanoids=float(p[7]),\n",
    "                  proanthocyanins=float(p[8]),\n",
    "                  color=float(p[9]),\n",
    "                  hue=float(p[10]),\n",
    "                  od=float(p[11]),\n",
    "                  proline=float(p[12])))\n",
    "wines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can now be converted to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(wines)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Spark DataFrame can act like a standard database, allowing for operations such as grouping. We can see the distribution of classes in this dataset by grouping on the `alcohol` label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"alcohol\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the DataFrame also allows for transformation and action operations, like RDDs. Since we have multiple columns, we can specify which columns to apply the operations to. For example, we can filter based on certain features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df[\"ash\"] <= 3.0) & (df[\"alcohol\"] >= 1)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly use SQL to query a Spark DataFrame. We first need to register the DataFrame in the Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"wines\")\n",
    "spark.sql(\"SELECT alcohol, ash FROM wines WHERE ash <= 3.0 AND alcohol >= 1\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider feature scaling the `ash` column. We'll do this by hand, but Spark ML provides the [StandardScaler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StandardScaler) to do it more easily. First, we aggregate the min and max values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ash = df.agg({\"ash\": \"max\"}).collect()[0][\"max(ash)\"]\n",
    "min_ash = df.agg({\"ash\": \"min\"}).collect()[0][\"min(ash)\"]\n",
    "max_ash, min_ash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a new function and register it. The `udf` module allows us to define any function to apply to our data while still using the Spark SQL framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf(\"double\")\n",
    "def normalize(s):\n",
    "  return (s - min_ash) / (max_ash - min_ash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed = df.select(\"ash\", normalize(\"ash\").alias(\"ash_normed\"))\n",
    "normed.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new DataFrame with the normalized ash values. Here we've been using a small dataset for an example, but the true benefits of Spark show when we're using large datasets distributed over many nodes. Everything we've just shown would be possible with distributed data, and Spark will automatically parallelize certain processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Spark is a general computing framework which excels at performance and is useful for a variety of tasks. While we briefly mentioned its `ml` package, it was not specifically developped for ML. Next, we'll look at Dask, which is another full Python solution for scaling data processing and ML tasks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_count(games):\n",
    "    return np.unique(games, return_counts=True)[1].tolist() # TODO: remove\n",
    "\n",
    "def heads_filter(counts):\n",
    "    return counts[0] > n_throws / 2 # TODO: remove\n",
    "\n",
    "def tails_filter(counts):\n",
    "    return counts[1] > n_throws / 2 # TODO: remove"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
